Convolutional Neural Network (CNN) that detects handwritten digits.

Previously, I had created an MLP network that performed the same task.

Now I will make the transition from MLP to CNN with the purpose of improving accuracy.

Typical CNN Architecture

Input (image) </br>
       ↓</br>
Convolution + ReLU</br>
       ↓</br>
Pooling</br>
       ↓</br>
Convolution + ReLU</br>
       ↓</br>
Pooling</br>
       ↓</br>
Flatten</br>
       ↓</br>
Fully Connected</br>
       ↓</br>
Softmax (classification)</br>
</br>
<h3>While in an MLP we initialized weights randomly and adjusted them using backpropagation, here what will be initialized and adjusted are the Convolution kernels*.</h3>

![alt text](miscellaneous/image.png)

Although the formula may seem complex, convolutions can be explained in a simple way — at least for the purposes of the topic at hand.

We have an image that will be translated into a map with values between 0 and 1:

    x_train = train.drop(columns=['label']).values / 255.0
    x_train = x_train.reshape(-1, 28, 28, 1)


We observe that, just like in the MLP, I take the data from the CSV, remove the label column, and divide by the maximum value (255) to normalize the data on a scale between 0 and 1 representing the grayscale of the image.  
Then, with reshape, I transform this array of values into 28×28 matrices. Convolution will be applied on this map.

Now then. As an example, I will show the convolution process using a 3×3 matrix instead of 28×28, with a 2×2 kernel.

I = input image (matrix)</br>
K = kernel/filter (small matrix, example 3×3)</br>
Y = feature map</br>

Interpretation:

- Slide the kernel over the image</br>
- Multiply each element of the kernel by the corresponding pixel</br>
- Add everything → one number in the feature map</br>

I = [[1, 2, 0],</br>
     [0, 1, 3],</br>
     [1, 2, 2]]</br>

K = [[1, 0],</br>
     [0, -1]]</br>

First step (top-left):</br>
1×1 + 2×0 + 0×0 + 1×(–1) = 1 + 0 + 0 – 1 = 0</br>
Second step (top-right):</br>
2×1 + 0×0 + 1×0 + 3×(–1) = 2 + 0 + 0 – 3 = –1</br>
Third step (bottom-left):</br>
0×1 + 1×0 + 1×0 + 2×(–1) = 0 + 0 + 0 – 2 = –2</br>
Fourth step (bottom-right):</br>
1×1 + 3×0 + 2×0 + 2×(–1) = 1 + 0 + 0 – 2 = –1</br>

Y = [[0, –1],</br>
     [–2, –1]]</br>

In our algorithm, K will be initialized with random values and adjusted through backpropagation to obtain the desired results during training.

In our layer structure model = Sequential([]) we observe:

    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),


TensorFlow will handle the convolutions by applying 32 kernels of size 3×3 over the 28×28 input, which is our image.

Doing the math, from a 28×28 matrix with 3×3 kernels applied, we obtain 26×26 outputs.

To these outputs we apply the MaxPooling2D function:

    MaxPooling2D((2,2))


This function scans the 26×26 output in 2×2 quadrants, and for each quadrant takes the maximum value to construct a new output matrix.

For a 26×26 matrix, MaxPooling2D((2,2)) results in a 13×13 output.

A smaller example:

[[1, 3, 2, 4],</br>
 [5, 6, 1, 2],</br>
 [0, 2, 3, 1],</br>
 [1, 0, 2, 4]]</br>

2×2 Pooling:

[[6, 4],</br>
 [2, 4]]</br>

From the 32 maps generated by MaxPooling, another convolution is applied with 63 kernels of size 3×3, followed by another MaxPooling 2×2. The result is 63 maps of size 5×5.

    Conv2D(63, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),


We can — and it is more useful — think not in terms of 63 maps of 5×5, but rather one three-dimensional matrix of 63×5×5.  
This gives a total of 1575 values. 63×5×5 = 1575.

Flatten() will place those 1575 values into a single 1D vector. And the Dense function will connect those 1575 values to 128 ReLU-activated neurons**.

    Flatten(),
    Dense(128, activation='relu'),


Now then. An important difference between the previous MLP network and this CNN is that the MLP does not interpret the visual shapes of the data it reads. MLP processes values as statistics.  
But CNN goes further and uses statistics to try to “see” the shapes in the images… It tries to interpret if the image has a curve, a straight line, circles, etc. And in this way, it infers the result.

For this same reason, a technique called Dropout is used to randomly turn off a percentage of the 128 neurons in the hidden layer prior to the output.

    Dropout(0.5)


In our case, half of those 128 neurons will be turned off, meaning inference will rely on 64 neurons.

Why do we need this? Because being a network that *does* take visual shapes into account, it could overfit its weights to always detect very repeated patterns in the digits it processes. That way, it would not generalize well to variations.

Imagine the training data teaches the network that an 8 is always a circle connected to another circle, both vertically aligned. Then the network will overfit the weights that capture that pattern.  
It will rely too much on those same weights.  
Basically, it won’t know how to read variations where an eight is drawn with one or both circles open or slightly tilted — typical human handwriting variations.

By randomly turning off half of its neurons, we ensure it does not always train the same weights, preventing overfitting.

Running the model produced a 99.19% accuracy.

</br>
</br>
Total params: 221,545 (865.41 KB)</br>
Trainable params: 221,545 (865.41 KB)</br>
Non-trainable params: 0 (0.00 B)</br>
Epoch 1/10</br>
accuracy: 0.9187 - loss: 0.2621 - val_accuracy: 0.9785 - val_loss: 0.0688</br>
Epoch 2/10</br>
accuracy: 0.9728 - loss: 0.0904 - val_accuracy: 0.9854 - val_loss: 0.0433</br>
Epoch 3/10</br>
accuracy: 0.9803 - loss: 0.0686 - val_accuracy: 0.9864 - val_loss: 0.0419</br>
Epoch 4/10</br>
accuracy: 0.9827 - loss: 0.0562 - val_accuracy: 0.9892 - val_loss: 0.0340</br>
Epoch 5/10</br>
accuracy: 0.9860 - loss: 0.0457 - val_accuracy: 0.9886 - val_loss: 0.0376</br>
Epoch 6/10</br>
accuracy: 0.9881 - loss: 0.0389 - val_accuracy: 0.9907 - val_loss: 0.0333</br>
Epoch 7/10</br>
accuracy: 0.9883 - loss: 0.0371 - val_accuracy: 0.9899 - val_loss: 0.0308</br>
Epoch 8/10</br>
accuracy: 0.9897 - loss: 0.0309 - val_accuracy: 0.9868 - val_loss: 0.0412</br>
Epoch 9/10</br>
accuracy: 0.9914 - loss: 0.0263 - val_accuracy: 0.9908 - val_loss: 0.0330</br>
Epoch 10/10</br>
accuracy: 0.9920 - loss: 0.0244 - val_accuracy: 0.9919 - val_loss: 0.0321</br>
</br>
</br>
*https://en.wikipedia.org/wiki/Convolution

**See README.md of the MLP Neural Network (https://github.com/Nahuel77/Red_Neuronal_MLP).
